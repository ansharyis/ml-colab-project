{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf0I7T88nDMtC6afstAwyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ansharyis/ml-colab-project/blob/main/notebooks/05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0r_QGGkwFcx",
        "outputId": "34015d6a-ce86-434d-9cd6-c2504814d756"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/drive/MyDrive/ML_Project_Data\""
      ],
      "metadata": {
        "id": "XuEcZBTOuIUq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load tha dataset"
      ],
      "metadata": {
        "id": "XZJqJqnqwSac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3EmAaVHpnqV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3266454-afbf-4436-9052-b0805ccb73d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20340, 59)\n"
          ]
        }
      ],
      "source": [
        "# Project: ML Weight Prediction\n",
        "# Notebook: Ensemble Model (Bagging and Boosting)\n",
        "# Owner: ....\n",
        "# Description: Ensemble Model (Bagging and Boosting) Rev A\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "train_df = pd.read_csv(f\"{DATA_DIR}/PROCESSED/train_df_final_after_null_removal.csv\")\n",
        "print(train_df.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "MVoA1oSYxAAS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare explanatory variable, dependent variable, and preprocessing pipeline\n",
        "\n",
        "This handles:\n",
        "\n",
        "numeric: median impute + scaling (good for linear/ridge/lasso)\n",
        "\n",
        "categorical: most_frequent + one-hot"
      ],
      "metadata": {
        "id": "KuTvNbD0y_Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = \"WEIGHTLBTC_A\"\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1) Prepare X / y\n",
        "# -------------------------------------------------\n",
        "df_model = train_df.dropna(subset=[TARGET]).copy()\n",
        "X = df_model.drop(columns=[TARGET])\n",
        "\n",
        "USE_LOG_TARGET = True  # keep True for inverse transform to pounds\n",
        "\n",
        "if USE_LOG_TARGET:\n",
        "    y = np.log1p(df_model[TARGET].astype(float))\n",
        "else:\n",
        "    y = df_model[TARGET].astype(float)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2) Detect numeric vs categorical columns\n",
        "# -------------------------------------------------\n",
        "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "num_cols = [c for c in X.columns if c not in cat_cols]\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3) Tree-friendly preprocessing (no scaling)\n",
        "# -------------------------------------------------\n",
        "categorical_tree = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(\n",
        "        handle_unknown=\"ignore\",\n",
        "        sparse_output=True,\n",
        "        min_frequency=0.01   # remove if sklearn doesn't support it\n",
        "    ))\n",
        "])\n",
        "\n",
        "numeric_tree = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
        "])\n",
        "\n",
        "tree_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", categorical_tree, cat_cols),\n",
        "        (\"num\", numeric_tree, num_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4) Train/test split\n",
        "# -------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=44\n",
        ")\n"
      ],
      "metadata": {
        "id": "bET1M5qixF_6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENSEMBLE MODEL"
      ],
      "metadata": {
        "id": "7BsJQe6Ky8ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------\n",
        "# 5) Define Ensemble Models (NO tuning)\n",
        "# -------------------------------------------------\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gb_model = GradientBoostingRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"RandomForest\": rf_model,\n",
        "    \"GradientBoosting\": gb_model\n",
        "}\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6) Train + Evaluate\n",
        "# -------------------------------------------------\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name} (no GridSearch)...\")\n",
        "\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"preprocessor\", tree_preprocessor),\n",
        "        (\"model\", model)\n",
        "    ])\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    if USE_LOG_TARGET:\n",
        "        y_pred_pounds = np.expm1(y_pred)\n",
        "        y_test_pounds = np.expm1(y_test)\n",
        "    else:\n",
        "        y_pred_pounds = y_pred\n",
        "        y_test_pounds = y_test\n",
        "\n",
        "    mse = mean_squared_error(y_test_pounds, y_pred_pounds)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    results.append({\"model\": name, \"RMSE\": rmse, \"MSE\": mse})\n",
        "    print(f\"{name} RMSE: {rmse:.2f} lbs\")\n",
        "    print(f\"{name} MSE : {mse:.2f} lbs^2\")\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(\"RMSE\")\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "dTioY5wFxMRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bcc078-f768-4416-b8fc-02c357153340"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training RandomForest (no GridSearch)...\n",
            "RandomForest RMSE: 15.67 lbs\n",
            "RandomForest MSE : 245.60 lbs^2\n",
            "\n",
            "Training GradientBoosting (no GridSearch)...\n",
            "GradientBoosting RMSE: 15.59 lbs\n",
            "GradientBoosting MSE : 243.16 lbs^2\n",
            "\n",
            "=== Summary ===\n",
            "              model       RMSE         MSE\n",
            "1  GradientBoosting  15.593496  243.157121\n",
            "0      RandomForest  15.671546  245.597365\n"
          ]
        }
      ]
    }
  ]
}